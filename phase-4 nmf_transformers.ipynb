{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84658d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPU] Using: NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "[DATA] 5842 docs, 11554 vocab\n",
      "[NMF] Iter 50, Loss=90.5770\n",
      "[NMF] Iter 100, Loss=90.5362\n",
      "[NMF] Iter 150, Loss=90.5292\n",
      "[NMF] Iter 200, Loss=90.5245\n",
      "Epoch 5 Loss=0.7491 Acc=61.26%\n",
      "Epoch 10 Loss=0.7466 Acc=62.08%\n",
      "Epoch 15 Loss=0.7448 Acc=62.60%\n",
      "Epoch 20 Loss=0.7434 Acc=62.89%\n",
      "Epoch 25 Loss=0.7421 Acc=63.08%\n",
      "Epoch 30 Loss=0.7410 Acc=63.09%\n",
      "Epoch 35 Loss=0.7401 Acc=62.96%\n",
      "Epoch 40 Loss=0.7393 Acc=62.89%\n",
      "Epoch 45 Loss=0.7388 Acc=62.89%\n",
      "Epoch 50 Loss=0.7384 Acc=62.86%\n",
      "Epoch 55 Loss=0.7382 Acc=62.86%\n",
      "Epoch 60 Loss=0.7382 Acc=62.96%\n",
      "Epoch 65 Loss=0.7384 Acc=62.96%\n",
      "Epoch 70 Loss=0.7387 Acc=62.87%\n",
      "Epoch 75 Loss=0.7393 Acc=62.86%\n",
      "Epoch 80 Loss=0.7400 Acc=62.79%\n",
      "Epoch 85 Loss=0.7410 Acc=62.68%\n",
      "Epoch 90 Loss=0.7421 Acc=62.63%\n",
      "Epoch 95 Loss=0.7434 Acc=62.31%\n",
      "Epoch 100 Loss=0.7449 Acc=62.24%\n",
      "Epoch 105 Loss=0.7466 Acc=62.14%\n",
      "Epoch 110 Loss=0.7485 Acc=62.03%\n",
      "Epoch 115 Loss=0.7506 Acc=61.90%\n",
      "Epoch 120 Loss=0.7529 Acc=61.74%\n",
      "Epoch 125 Loss=0.7553 Acc=61.74%\n",
      "Epoch 130 Loss=0.7580 Acc=61.50%\n",
      "Epoch 135 Loss=0.7608 Acc=61.28%\n",
      "Epoch 140 Loss=0.7639 Acc=61.23%\n",
      "Epoch 145 Loss=0.7671 Acc=60.99%\n",
      "Epoch 150 Loss=0.7705 Acc=61.02%\n",
      "Epoch 155 Loss=0.7741 Acc=60.96%\n",
      "Epoch 160 Loss=0.7779 Acc=60.87%\n",
      "Epoch 165 Loss=0.7818 Acc=60.78%\n",
      "Epoch 170 Loss=0.7860 Acc=60.54%\n",
      "Epoch 175 Loss=0.7903 Acc=60.49%\n",
      "Epoch 180 Loss=0.7948 Acc=60.41%\n",
      "Epoch 185 Loss=0.7995 Acc=60.25%\n",
      "Epoch 190 Loss=0.8043 Acc=60.01%\n",
      "Epoch 195 Loss=0.8093 Acc=59.91%\n",
      "Epoch 200 Loss=0.8145 Acc=59.77%\n",
      "Epoch 205 Loss=0.8199 Acc=59.74%\n",
      "Epoch 210 Loss=0.8254 Acc=59.69%\n",
      "Epoch 215 Loss=0.8311 Acc=59.52%\n",
      "Epoch 220 Loss=0.8369 Acc=59.36%\n",
      "Epoch 225 Loss=0.8429 Acc=59.41%\n",
      "Epoch 230 Loss=0.8491 Acc=59.35%\n",
      "Epoch 235 Loss=0.8554 Acc=59.17%\n",
      "Epoch 240 Loss=0.8619 Acc=59.07%\n",
      "Epoch 245 Loss=0.8685 Acc=59.00%\n",
      "Epoch 250 Loss=0.8752 Acc=58.99%\n",
      "Epoch 255 Loss=0.8821 Acc=58.97%\n",
      "Epoch 260 Loss=0.8892 Acc=58.76%\n",
      "Epoch 265 Loss=0.8963 Acc=58.70%\n",
      "Epoch 270 Loss=0.9037 Acc=58.44%\n",
      "Epoch 275 Loss=0.9111 Acc=58.44%\n",
      "Epoch 280 Loss=0.9187 Acc=58.23%\n",
      "Epoch 285 Loss=0.9264 Acc=58.13%\n",
      "Epoch 290 Loss=0.9342 Acc=58.03%\n",
      "Epoch 295 Loss=0.9422 Acc=57.98%\n",
      "Epoch 300 Loss=0.9502 Acc=58.03%\n",
      "Epoch 305 Loss=0.9584 Acc=57.87%\n",
      "Epoch 310 Loss=0.9667 Acc=57.79%\n",
      "Epoch 315 Loss=0.9751 Acc=57.82%\n",
      "Epoch 320 Loss=0.9836 Acc=57.72%\n",
      "Epoch 325 Loss=0.9923 Acc=57.58%\n",
      "Epoch 330 Loss=1.0010 Acc=57.55%\n",
      "Epoch 335 Loss=1.0098 Acc=57.50%\n",
      "Epoch 340 Loss=1.0188 Acc=57.53%\n",
      "Epoch 345 Loss=1.0278 Acc=57.57%\n",
      "Epoch 350 Loss=1.0369 Acc=57.60%\n",
      "Epoch 355 Loss=1.0461 Acc=57.58%\n",
      "Epoch 360 Loss=1.0554 Acc=57.50%\n",
      "Epoch 365 Loss=1.0648 Acc=57.50%\n",
      "Epoch 370 Loss=1.0743 Acc=57.55%\n",
      "Epoch 375 Loss=1.0839 Acc=57.46%\n",
      "Epoch 380 Loss=1.0935 Acc=57.46%\n",
      "Epoch 385 Loss=1.1032 Acc=57.41%\n",
      "Epoch 390 Loss=1.1130 Acc=57.38%\n",
      "Epoch 395 Loss=1.1229 Acc=57.33%\n",
      "Epoch 400 Loss=1.1328 Acc=57.31%\n",
      "Epoch 405 Loss=1.1429 Acc=57.22%\n",
      "Epoch 410 Loss=1.1529 Acc=57.10%\n",
      "Epoch 415 Loss=1.1631 Acc=57.16%\n",
      "Epoch 420 Loss=1.1733 Acc=57.10%\n",
      "Epoch 425 Loss=1.1836 Acc=57.14%\n",
      "Epoch 430 Loss=1.1940 Acc=57.09%\n",
      "Epoch 435 Loss=1.2044 Acc=57.00%\n",
      "Epoch 440 Loss=1.2149 Acc=56.98%\n",
      "Epoch 445 Loss=1.2254 Acc=56.98%\n",
      "Epoch 450 Loss=1.2360 Acc=56.97%\n",
      "Epoch 455 Loss=1.2466 Acc=56.86%\n",
      "Epoch 460 Loss=1.2573 Acc=56.73%\n",
      "Epoch 465 Loss=1.2681 Acc=56.69%\n",
      "Epoch 470 Loss=1.2789 Acc=56.52%\n",
      "Epoch 475 Loss=1.2897 Acc=56.52%\n",
      "Epoch 480 Loss=1.3006 Acc=56.49%\n",
      "Epoch 485 Loss=1.3116 Acc=56.49%\n",
      "Epoch 490 Loss=1.3226 Acc=56.42%\n",
      "Epoch 495 Loss=1.3336 Acc=56.35%\n",
      "Epoch 500 Loss=1.3447 Acc=56.23%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "import pandas as pd\n",
    "\n",
    "assert cp.cuda.runtime.getDeviceCount() > 0, \"No CUDA GPU detected!\"\n",
    "with cp.cuda.Device(0) as dev:\n",
    "    props = cp.cuda.runtime.getDeviceProperties(dev.id)\n",
    "    print(f\"[GPU] Using: {props['name'].decode() if isinstance(props['name'], bytes) else props['name']}\")\n",
    "\n",
    "def clean_text(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[^\\w\\s$]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    vocab = {}\n",
    "    for s in sentences:\n",
    "        for w in s.split():\n",
    "            if w not in vocab:\n",
    "                vocab[w] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "# TFâ€“IDF from scratch (GPU)\n",
    "\n",
    "def compute_tfidf_gpu(sentences, vocab):\n",
    "    n_docs = len(sentences)\n",
    "    vocab_size = len(vocab)\n",
    "    mat = cp.zeros((n_docs, vocab_size), dtype=cp.float32)\n",
    "\n",
    "    for i, s in enumerate(sentences):\n",
    "        words = s.split()\n",
    "        for w in words:\n",
    "            if w in vocab:\n",
    "                mat[i, vocab[w]] += 1.0\n",
    "\n",
    "    # term frequency normalization\n",
    "    row_sums = cp.sum(mat, axis=1, keepdims=True) + 1e-9\n",
    "    tf = mat / row_sums\n",
    "\n",
    "    # inverse document frequency\n",
    "    df = cp.sum(mat > 0, axis=0) + 1.0\n",
    "    idf = cp.log((n_docs + 1.0) / df)\n",
    "\n",
    "    tfidf = tf * idf\n",
    "    return tfidf\n",
    "\n",
    "# NMF Multiplicative Updates \n",
    "\n",
    "def nmf_gpu(V, rank=50, iters=200):\n",
    "    m, n = V.shape\n",
    "    W = cp.random.rand(m, rank).astype(cp.float32)\n",
    "    H = cp.random.rand(rank, n).astype(cp.float32)\n",
    "\n",
    "    for i in range(iters):\n",
    "        # Update H\n",
    "        WH = W @ H\n",
    "        H *= (W.T @ V) / (W.T @ WH + 1e-9)\n",
    "\n",
    "        # Update W\n",
    "        WH = W @ H\n",
    "        W *= (V @ H.T) / (W @ (H @ H.T) + 1e-9)\n",
    "\n",
    "        if (i+1) % 50 == 0:\n",
    "            loss = cp.linalg.norm(V - W @ H)\n",
    "            print(f\"[NMF] Iter {i+1}, Loss={loss:.4f}\")\n",
    "    return W, H\n",
    "\n",
    "\n",
    "# kernels\n",
    "\n",
    "matmul_kernel_code = r'''\n",
    "extern \"C\" __global__\n",
    "void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N){\n",
    "    int row = blockDim.y * blockIdx.y + threadIdx.y;\n",
    "    int col = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "    if(row < M && col < N){\n",
    "        float sum = 0.0f;\n",
    "        for(int k = 0; k < K; ++k){\n",
    "            sum += A[row*K + k] * B[k*N + col];\n",
    "        }\n",
    "        C[row*N + col] = sum;\n",
    "    }\n",
    "}\n",
    "'''\n",
    "matmul_kernel = cp.RawKernel(matmul_kernel_code, 'matmul_kernel')\n",
    "\n",
    "def matmul_manual(A, B):\n",
    "    M, K = A.shape\n",
    "    K2, N = B.shape\n",
    "    assert K == K2\n",
    "    C = cp.zeros((M, N), dtype=cp.float32)\n",
    "    block = (16, 16, 1)\n",
    "    grid = ((N+15)//16, (M+15)//16, 1)\n",
    "    matmul_kernel(grid, block, (A, B, C, M, K, N))\n",
    "    return C\n",
    "\n",
    "softmax_kernel_code = r'''\n",
    "extern \"C\" __global__\n",
    "void row_softmax(float* X, int M, int N){\n",
    "    int row = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "    if(row < M){\n",
    "        float max_val = -1e20f;\n",
    "        for(int j=0;j<N;++j){\n",
    "            float v = X[row*N+j];\n",
    "            max_val = v > max_val ? v : max_val;\n",
    "        }\n",
    "        float sum_exp=0.0f;\n",
    "        for(int j=0;j<N;++j){\n",
    "            float e = __expf(X[row*N+j]-max_val);\n",
    "            X[row*N+j] = e;\n",
    "            sum_exp += e;\n",
    "        }\n",
    "        float inv=1.0f/sum_exp;\n",
    "        for(int j=0;j<N;++j){\n",
    "            X[row*N+j]*=inv;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "'''\n",
    "softmax_kernel = cp.RawKernel(softmax_kernel_code, 'row_softmax')\n",
    "\n",
    "def softmax_manual(X):\n",
    "    M,N = X.shape\n",
    "    block=(128,1,1)\n",
    "    grid=((M+127)//128,1,1)\n",
    "    softmax_kernel(grid,block,(X,M,N))\n",
    "    return X\n",
    "\n",
    "relu_kernel_code = r'''\n",
    "extern \"C\" __global__\n",
    "void relu(float* X, int MN){\n",
    "    int idx = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "    if(idx < MN){\n",
    "        float v=X[idx];\n",
    "        X[idx]=v>0.0f?v:0.0f;\n",
    "    }\n",
    "}\n",
    "'''\n",
    "relu_kernel = cp.RawKernel(relu_kernel_code,'relu')\n",
    "\n",
    "def relu_manual(X):\n",
    "    MN=X.size\n",
    "    block=(256,1,1)\n",
    "    grid=((MN+255)//256,1,1)\n",
    "    relu_kernel(grid,block,(X,MN))\n",
    "    return X\n",
    "\n",
    "# Transformer  \n",
    "class TransformerClassifierGPU:\n",
    "    def __init__(self, input_dim, n_classes, d_model=64):\n",
    "        self.W_embed = cp.random.randn(input_dim, d_model).astype(cp.float32)*0.1\n",
    "        self.W_Q = cp.random.randn(d_model,d_model).astype(cp.float32)*0.1\n",
    "        self.W_K = cp.random.randn(d_model,d_model).astype(cp.float32)*0.1\n",
    "        self.W_V = cp.random.randn(d_model,d_model).astype(cp.float32)*0.1\n",
    "        self.W_ff= cp.random.randn(d_model,d_model).astype(cp.float32)*0.1\n",
    "        self.b_ff= cp.zeros(d_model,dtype=cp.float32)\n",
    "        self.W_out=cp.random.randn(d_model,n_classes).astype(cp.float32)*0.1\n",
    "        self.b_out=cp.zeros(n_classes,dtype=cp.float32)\n",
    "\n",
    "    def forward(self,X):\n",
    "        X_emb=matmul_manual(X,self.W_embed)\n",
    "        Q=matmul_manual(X_emb,self.W_Q)\n",
    "        K=matmul_manual(X_emb,self.W_K)\n",
    "        V=matmul_manual(X_emb,self.W_V)\n",
    "        scores=matmul_manual(Q,K.T.copy())/cp.sqrt(cp.float32(Q.shape[1]))\n",
    "        attn=softmax_manual(scores.copy())\n",
    "        attn_out=matmul_manual(attn,V)\n",
    "        X_res=X_emb+attn_out\n",
    "        mean=cp.mean(X_res,axis=1,keepdims=True)\n",
    "        std=cp.std(X_res,axis=1,keepdims=True)+1e-6\n",
    "        X_norm=(X_res-mean)/std\n",
    "        FF=matmul_manual(X_norm,self.W_ff)+self.b_ff\n",
    "        FF=relu_manual(FF)\n",
    "        X_ff=X_norm+FF\n",
    "        logits=matmul_manual(X_ff,self.W_out)+self.b_out\n",
    "        return logits,X_ff\n",
    "\n",
    "    def update(self,X_ff,y,logits,lr):\n",
    "        exp_logits=cp.exp(logits-cp.max(logits,axis=1,keepdims=True))\n",
    "        probs=exp_logits/cp.sum(exp_logits,axis=1,keepdims=True)\n",
    "        one_hot=cp.zeros_like(probs)\n",
    "        one_hot[cp.arange(y.size),y]=1.0\n",
    "        grad_logits=(probs-one_hot)/y.size\n",
    "        grad_W=matmul_manual(X_ff.T,grad_logits)\n",
    "        grad_b=cp.sum(grad_logits,axis=0)\n",
    "        self.W_out-=lr*grad_W\n",
    "        self.b_out-=lr*grad_b\n",
    "\n",
    "def cross_entropy_loss(logits,y):\n",
    "    maxl=cp.max(logits,axis=1,keepdims=True)\n",
    "    log_probs=logits-maxl-cp.log(cp.sum(cp.exp(logits-maxl),axis=1,keepdims=True))\n",
    "    return -cp.mean(log_probs[cp.arange(y.size),y])\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    df=pd.read_csv(r\"C:\\Users\\Pavani Akshaya\\Downloads\\fin_data_1.csv\")\n",
    "    sentences=[clean_text(s) for s in df[\"Sentence\"].astype(str).tolist()]\n",
    "    labels=df[\"Sentiment\"].astype(str).str.lower().map({\"positive\":1,\"pos\":1,\"1\":1,\n",
    "                                                        \"negative\":0,\"neg\":0,\"0\":0}).fillna(0).astype(int).to_numpy()\n",
    "\n",
    "    vocab=build_vocab(sentences)\n",
    "    print(f\"[DATA] {len(sentences)} docs, {len(vocab)} vocab\")\n",
    "\n",
    "    tfidf=compute_tfidf_gpu(sentences,vocab)\n",
    "    W,H=nmf_gpu(tfidf,rank=64,iters=200)\n",
    "    X_gpu=W/(cp.linalg.norm(W,axis=1,keepdims=True)+1e-9)\n",
    "\n",
    "    n_classes=len(set(labels))\n",
    "    model=TransformerClassifierGPU(X_gpu.shape[1],n_classes)\n",
    "\n",
    "    y_gpu=cp.asarray(labels)\n",
    "    for epoch in range(500):\n",
    "        logits,X_ff=model.forward(X_gpu)\n",
    "        loss=cross_entropy_loss(logits,y_gpu)\n",
    "        model.update(X_ff,y_gpu,logits,0.05)\n",
    "        if (epoch+1)%5==0:\n",
    "            acc=cp.mean((cp.argmax(logits,axis=1)==y_gpu).astype(cp.float32))\n",
    "            print(f\"Epoch {epoch+1} Loss={loss:.4f} Acc={float(acc)*100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpuenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
