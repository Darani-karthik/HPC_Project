{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bde92215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Before Oversampling] Class distribution:\n",
      "Class 0: 679\n",
      "Class 1: 2537\n",
      "Class 2: 1457\n",
      "\n",
      "[After Oversampling] Class distribution:\n",
      "Class 0: 2537\n",
      "Class 1: 2537\n",
      "Class 2: 2537\n",
      "Epoch 50 Loss=1.2835 Train Acc=34.24%\n",
      "Epoch 100 Loss=1.2746 Train Acc=34.29%\n",
      "\n",
      "=== Final Test Accuracy: 34.39% ===\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# trial -1 \n",
    "\n",
    "#Shallow Transformer network   with the onehot encoding embeddings \n",
    "import re\n",
    "import cupy as cp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def clean_text(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[^a-z\\s]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    vocab = {}\n",
    "    for s in sentences:\n",
    "        for w in s.split():\n",
    "            if w not in vocab:\n",
    "                vocab[w] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "def encode_sentences_onehot(sentences, vocab):\n",
    "    n_docs = len(sentences)\n",
    "    vocab_size = len(vocab)\n",
    "    mat = np.zeros((n_docs, vocab_size), dtype=np.float32)\n",
    "    for i, s in enumerate(sentences):\n",
    "        for w in s.split():\n",
    "            if w in vocab:\n",
    "                mat[i, vocab[w]] = 1.0\n",
    "    return mat\n",
    "\n",
    "def balance_classes(X, y):\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    max_count = np.max(counts)\n",
    "    new_X, new_y = [], []\n",
    "    for cls in unique:\n",
    "        cls_idx = np.where(y == cls)[0]\n",
    "        cls_samples = X[cls_idx]\n",
    "        cls_labels  = y[cls_idx]\n",
    "        reps = max_count // len(cls_samples)\n",
    "        rem  = max_count % len(cls_samples)\n",
    "        new_X.append(np.tile(cls_samples, (reps,1)))\n",
    "        new_y.append(np.tile(cls_labels, reps))\n",
    "        if rem > 0:\n",
    "            choice = np.random.choice(len(cls_samples), rem, replace=True)\n",
    "            new_X.append(cls_samples[choice])\n",
    "            new_y.append(cls_labels[choice])\n",
    "    return np.vstack(new_X), np.hstack(new_y)\n",
    "\n",
    "# CUDA kernels\n",
    "matmul_kernel_code = r'''\n",
    "extern \"C\" __global__\n",
    "void matmul_kernel(const float* A, const float* B, float* C, int M, int K, int N){\n",
    "    int row = blockDim.y * blockIdx.y + threadIdx.y;\n",
    "    int col = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "    if(row < M && col < N){\n",
    "        float sum = 0.0f;\n",
    "        for(int k = 0; k < K; ++k){\n",
    "            sum += A[row*K + k] * B[k*N + col];\n",
    "        }\n",
    "        C[row*N + col] = sum;\n",
    "    }\n",
    "}\n",
    "'''\n",
    "matmul_kernel = cp.RawKernel(matmul_kernel_code, 'matmul_kernel')\n",
    "\n",
    "def matmul_manual(A, B):\n",
    "    M, K = A.shape\n",
    "    K2, N = B.shape\n",
    "    assert K == K2\n",
    "    C = cp.zeros((M, N), dtype=cp.float32)\n",
    "    block = (16, 16, 1)\n",
    "    grid = ((N+15)//16, (M+15)//16, 1)\n",
    "    matmul_kernel(grid, block, (A, B, C, M, K, N))\n",
    "    return C\n",
    "\n",
    "softmax_kernel_code = r'''\n",
    "extern \"C\" __global__\n",
    "void row_softmax(float* X, int M, int N){\n",
    "    int row = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "    if(row < M){\n",
    "        float max_val = -1e20f;  \n",
    "        for(int j=0;j<N;++j){\n",
    "            float v = X[row*N+j];\n",
    "            max_val = v > max_val ? v : max_val;\n",
    "        } \n",
    "        float sum_exp=0.0f;\n",
    "        for(int j=0;j<N;++j){\n",
    "            float e = __expf(X[row*N+j]-max_val);\n",
    "            X[row*N+j] = e;\n",
    "            sum_exp += e;\n",
    "        }\n",
    "        float inv=1.0f/sum_exp;\n",
    "        for(int j=0;j<N;++j){\n",
    "            X[row*N+j]*=inv;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "'''\n",
    "softmax_kernel = cp.RawKernel(softmax_kernel_code, 'row_softmax')\n",
    "\n",
    "def softmax_manual(X):\n",
    "    M,N = X.shape\n",
    "    block=(128,1,1)\n",
    "    grid=((M+127)//128,1,1)\n",
    "    softmax_kernel(grid,block,(X,M,N))\n",
    "    return X\n",
    "\n",
    "relu_kernel_code = r'''\n",
    "extern \"C\" __global__\n",
    "void relu(float* X, int MN){\n",
    "    int idx = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "    if(idx < MN){\n",
    "        float v=X[idx];\n",
    "        X[idx]=v>0.0f?v:0.0f;\n",
    "    }\n",
    "}\n",
    "'''\n",
    "relu_kernel = cp.RawKernel(relu_kernel_code,'relu')\n",
    "\n",
    "def relu_manual(X):\n",
    "    MN=X.size\n",
    "    block=(256,1,1)\n",
    "    grid=((MN+255)//256,1,1)\n",
    "    relu_kernel(grid,block,(X,MN))\n",
    "    return X\n",
    "\n",
    "# Transformer Classifier \n",
    "class TransformerClassifierGPU:\n",
    "    def __init__(self, input_dim, n_classes, d_model=64):\n",
    "        self.W_embed = cp.random.randn(input_dim, d_model).astype(cp.float32)*0.1\n",
    "        self.W_Q = cp.random.randn(d_model,d_model).astype(cp.float32)*0.1\n",
    "        self.W_K = cp.random.randn(d_model,d_model).astype(cp.float32)*0.1\n",
    "        self.W_V = cp.random.randn(d_model,d_model).astype(cp.float32)*0.1\n",
    "        self.W_ff= cp.random.randn(d_model,d_model).astype(cp.float32)*0.1\n",
    "        self.b_ff= cp.zeros(d_model,dtype=cp.float32)\n",
    "        self.W_out=cp.random.randn(d_model,n_classes).astype(cp.float32)*0.1\n",
    "        self.b_out=cp.zeros(n_classes,dtype=cp.float32)\n",
    "\n",
    "    def forward(self,X):\n",
    "        X_emb=matmul_manual(X,self.W_embed)\n",
    "        Q=matmul_manual(X_emb,self.W_Q)\n",
    "        K=matmul_manual(X_emb,self.W_K)\n",
    "        V=matmul_manual(X_emb,self.W_V)\n",
    "        scores=matmul_manual(Q,K.T.copy())/cp.sqrt(cp.float32(Q.shape[1]))\n",
    "        attn=softmax_manual(scores.copy())\n",
    "        attn_out=matmul_manual(attn,V)\n",
    "        X_res=X_emb+attn_out\n",
    "        mean=cp.mean(X_res,axis=1,keepdims=True)\n",
    "        std=cp.std(X_res,axis=1,keepdims=True)+1e-6\n",
    "        X_norm=(X_res-mean)/std\n",
    "        FF=matmul_manual(X_norm,self.W_ff)+self.b_ff\n",
    "        FF=relu_manual(FF)\n",
    "        X_ff=X_norm+FF\n",
    "        logits=matmul_manual(X_ff,self.W_out)+self.b_out\n",
    "        return logits,X_ff\n",
    "    \n",
    "    def update(self,X_ff,y,logits,lr):\n",
    "        exp_logits=cp.exp(logits-cp.max(logits,axis=1,keepdims=True))\n",
    "        probs=exp_logits/cp.sum(exp_logits,axis=1,keepdims=True)\n",
    "        one_hot=cp.zeros_like(probs)\n",
    "        one_hot[cp.arange(y.size),y]=1.0\n",
    "        grad_logits=(probs-one_hot)/y.size\n",
    "        grad_W=matmul_manual(X_ff.T,grad_logits)\n",
    "        grad_b=cp.sum(grad_logits,axis=0)\n",
    "        self.W_out-=lr*grad_W\n",
    "        self.b_out-=lr*grad_b\n",
    "#loss \n",
    "def cross_entropy_loss(logits,y):\n",
    "    maxl=cp.max(logits,axis=1,keepdims=True)\n",
    "    log_probs=logits-maxl-cp.log(cp.sum(cp.exp(logits-maxl),axis=1,keepdims=True))\n",
    "    return -cp.mean(log_probs[cp.arange(y.size),y])\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    df=pd.read_csv(r\"C:\\Users\\Pavani Akshaya\\Downloads\\fin_data_1.csv\")\n",
    "    sentences=[clean_text(s) for s in df[\"Sentence\"].astype(str).tolist()]\n",
    "\n",
    "    label_map={\n",
    "        \"positive\":2,\"pos\":2,\"1\":2,\n",
    "        \"neutral\":1,\"neu\":1,\"2\":1,\n",
    "        \"negative\":0,\"neg\":0,\"0\":0\n",
    "    }\n",
    "    labels=df[\"Sentiment\"].astype(str).str.lower().map(label_map)\n",
    "    \n",
    "    # Fallback: if unmapped, assign neutral (1)\n",
    "    labels=labels.fillna(1).astype(int).to_numpy()\n",
    "\n",
    "    idx = np.arange(len(sentences))\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(idx)\n",
    "    split = int(0.8*len(sentences))\n",
    "    train_idx, test_idx = idx[:split], idx[split:]\n",
    "    train_sent = [sentences[i] for i in train_idx]\n",
    "    test_sent  = [sentences[i] for i in test_idx]\n",
    "    train_lab  = labels[train_idx]\n",
    "    test_lab   = labels[test_idx]\n",
    "\n",
    "    vocab=build_vocab(train_sent)\n",
    "    X_train_cpu=encode_sentences_onehot(train_sent,vocab)\n",
    "    X_test_cpu=encode_sentences_onehot(test_sent,vocab)\n",
    "\n",
    "    print(\"\\n[Before Oversampling] Class distribution:\")\n",
    "    unique, counts = np.unique(train_lab, return_counts=True)\n",
    "    for u,c in zip(unique,counts):\n",
    "        print(f\"Class {u}: {c}\")\n",
    "\n",
    "    X_train_bal, y_train_bal = balance_classes(X_train_cpu, train_lab)\n",
    "\n",
    "    print(\"\\n[After Oversampling] Class distribution:\")\n",
    "    unique, counts = np.unique(y_train_bal, return_counts=True)\n",
    "    for u,c in zip(unique,counts):\n",
    "        print(f\"Class {u}: {c}\")\n",
    "\n",
    "    # Move to GPU\n",
    "    X_train=cp.asarray(X_train_bal)\n",
    "    X_test=cp.asarray(X_test_cpu)\n",
    "    y_train=cp.asarray(y_train_bal)\n",
    "    y_test=cp.asarray(test_lab)\n",
    "\n",
    "    model=TransformerClassifierGPU(X_train.shape[1],3)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(100):   \n",
    "        logits,X_ff=model.forward(X_train)\n",
    "        loss=cross_entropy_loss(logits,y_train)\n",
    "        model.update(X_ff,y_train,logits,0.01)\n",
    "        if (epoch+1)%50==0:\n",
    "            preds=cp.argmax(logits,axis=1)\n",
    "            acc=cp.mean((preds==y_train).astype(cp.float32))\n",
    "            print(f\"Epoch {epoch+1} Loss={loss:.4f} Train Acc={float(acc)*100:.2f}%\")\n",
    "\n",
    "    # Final test evaluation\n",
    "    logits,_=model.forward(X_test)\n",
    "    y_pred=cp.argmax(logits,axis=1)\n",
    "    acc=cp.mean((y_pred==y_test).astype(cp.float32))\n",
    "    print(f\"\\n=== Final Test Accuracy: {float(acc)*100:.2f}% ===\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpuenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
